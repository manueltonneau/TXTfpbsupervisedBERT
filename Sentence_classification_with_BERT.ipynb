{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Second attempt: sentence classification with BERT",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manueltonneau/TXTfpbsupervisedBERT/blob/master/Sentence_classification_with_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lNW0wFV9Y_b",
        "colab_type": "code",
        "outputId": "33506504-d831-4ee9-ee80-fc3ac9cc0347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install Cython"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMIifh_Bs7ra",
        "colab_type": "code",
        "outputId": "6ad5b31e-754c-4ec3-db2b-e59e65b7c50a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.8.19)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.33)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.216)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.83)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.13.2)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.216 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.216)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.216->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.216->boto3->pytorch-transformers) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijT6Z7Ei_Mz",
        "colab_type": "code",
        "outputId": "ee1435ea-62e5-414d-905f-6ff02779e321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import *\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1JScnXTjDhI",
        "colab_type": "code",
        "outputId": "87517f1d-ec32-4f6f-9814-4b8b9cf68c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        }
      },
      "source": [
        "!pip install pretrainedmodels\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install fastai==1.0.52\n",
        "import fastai\n",
        "\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.text import *\n",
        "\n",
        "from torchvision.models import *\n",
        "import pretrainedmodels\n",
        "!pip install utils\n",
        "\n",
        "from utils import *\n",
        "import sys\n",
        "\n",
        "from fastai.callbacks.tracker import EarlyStoppingCallback\n",
        "from fastai.callbacks.tracker import SaveModelCallback"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.3.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (2.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.16.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->pretrainedmodels) (0.46)\n",
            "Requirement already satisfied: fastai==1.0.52 in /usr/local/lib/python3.6/dist-packages (1.0.52)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (2.21.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (0.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (4.6.3)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (7.352.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (1.1.0)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (2.1.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (19.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (0.3.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (2.7.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (1.2.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (3.7.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (4.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (1.3.1)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (0.1.21)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (3.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (0.24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.52) (3.13)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.52) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.52) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.52) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.52) (1.24.3)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (0.2.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (0.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (0.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.52) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==1.0.52) (1.12.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==1.0.52) (19.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==1.0.52) (2.4.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai==1.0.52) (0.46)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.52) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.52) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.52) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==1.0.52) (2018.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai==1.0.52) (4.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai==1.0.52) (41.2.0)\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO3U_chujGx7",
        "colab_type": "code",
        "outputId": "373f602c-30c5-45a9-ff77-7943d251d390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "%%bash\n",
        "pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.216)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.8.19)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.216 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.216)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.216->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.216->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.216->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgsTH6smjJ3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "bert_tok = BertTokenizer.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqp-B0XIjX4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastAiBertTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length\"\"\"\n",
        "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S2WJab-jcqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG0dtbogjzP8",
        "colab_type": "code",
        "outputId": "f827c653-ca10-451f-c4d9-2466aa54d8a0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Upload the train file from your local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-00cff357-4b6f-4f82-9b4a-cc771f4ae0b1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-00cff357-4b6f-4f82-9b4a-cc771f4ae0b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving pre_processed_aapl_sentences.csv to pre_processed_aapl_sentences.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vy7jLjlE4kN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload the train file from your local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgAY5dszj4rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"Sentences_AllAgree_preprocessed.csv\")#, delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUGu5gCaD298",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(\"pre_processed_aapl_sentences.csv\",index_col=None, header=0, engine='python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0n79uIeM7H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhIpNnzhjkzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#DATA_ROOT = Path(\"..\") / \"input\"\n",
        "\n",
        "#train, test = [pd.read_csv(DATA_ROOT / fname) for fname in [\"train.csv\", \"test.csv\"]]\n",
        "train_1, val = train_test_split(train, shuffle=True, test_size=0.2, random_state=42)\n",
        "#train_1, val = train_test_split(train_1, shuffle=True, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTocHyYGj9MJ",
        "colab_type": "code",
        "outputId": "0cc656ba-1c7a-482e-a4b9-07b54ff04f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label_negative</th>\n",
              "      <th>label_neutral</th>\n",
              "      <th>label_positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...  label_positive\n",
              "0  According to Gran , the company has no plans t...  ...               0\n",
              "1  Technopolis plans to develop in stages an area...  ...               0\n",
              "2  With the new production plant the company woul...  ...               1\n",
              "3  According to the company 's updated strategy f...  ...               1\n",
              "4  For the last quarter of 2010 , Componenta 's n...  ...               1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICz0MJPqkDP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmFzbW_2kNwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRqvM31DkRE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "label_cols = [\"label_negative\",\"label_neutral\",\"label_positive\"]\n",
        "\n",
        "databunch_1 = TextDataBunch.from_df(\".\", train, val, \n",
        "                  tokenizer=fastai_tokenizer,\n",
        "                  vocab=fastai_bert_vocab,\n",
        "                  include_bos=False,\n",
        "                  include_eos=False,\n",
        "                  text_cols=\"sentence\",\n",
        "                  label_cols=label_cols,\n",
        "                  bs=32,\n",
        "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No2icuI1kTAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertTokenizeProcessor(TokenizeProcessor):\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
        "\n",
        "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    \"\"\"\n",
        "    Constructing preprocessors for BERT\n",
        "    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n",
        "    We also use a custom vocabulary to match the numericalization with the original BERT model.\n",
        "    \"\"\"\n",
        "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
        "            NumericalizeProcessor(vocab=vocab)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N4oVs7QnYQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertDataBunch(TextDataBunch):\n",
        "    @classmethod\n",
        "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
        "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
        "                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` from DataFrames.\"\n",
        "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
        "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
        "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
        "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
        "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
        "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
        "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
        "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
        "        return src.databunch(**kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JS3c2cMnari",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this will produce a virtually identical databunch to the code above\n",
        "databunch_2 = BertDataBunch.from_df(\".\", train_df=train, valid_df=val,\n",
        "                  tokenizer=fastai_tokenizer,\n",
        "                  vocab=fastai_bert_vocab,\n",
        "                  text_cols=\"sentence\",\n",
        "                  label_cols=label_cols,\n",
        "                  bs=32,\n",
        "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReR6-VHAncyt",
        "colab_type": "code",
        "outputId": "c0f80031-5bf9-4ec2-9c84-040075f174d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "databunch_1.show_batch()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[CLS] supported nokia phones include : n ##9 ##6 , n ##9 ##5 - 8 ##gb , n ##9 ##5 , n ##9 ##3 - n ##9 ##31 , n ##9 ##2 , n ##85 , n ##8 ##2 , n ##8 ##1 , n ##80 , n ##7 ##9 , n ##7 ##8 , n ##7 ##7 , n ##7 ##6 , n ##75 , n ##7 ##3 ,</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] narrows to eu ##r ##2 . 8 m 9 - mo ' 09 29 october 2009 - finnish software and hardware developer el ##ek ##tro ##bit o ##y ##j he ##l : e ##b ##g ##1 ##v , or e ##b , said today that its net loss narrowed to eu ##r ##2 . 8 m for the first nine months of 2009 from eu ##r ##35 . 6</td>\n",
              "      <td>label_positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] mana ##vi ##gat ##or - september 7 , 2010 - ke ##mir ##a un ##ve ##ils indian j ##v with iv ##rc ##l finnish chemicals group ke ##mir ##a ( he ##l : k ##ra ##1 ##v ) on tuesday announced it has ink ##ed a deal to form a joint venture in india with local construction firm iv ##rc ##l infrastructure and projects ltd ( bo ##m :</td>\n",
              "      <td>label_positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] pg ##e bel ##cha ##tow runs the 4 . 44 g ##w bel ##cha ##tow coal - fired power plant , and fort ##um has intentions to start a cc ##s demonstration project jointly with te ##oll ##is ##u ##ude ##n vo ##ima o ##y ##j ( tv ##o ) - another finnish utility - at the their jointly owned 56 ##5 ##m ##w mer ##i - por ##i</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8SRc6uenwcX",
        "colab_type": "text"
      },
      "source": [
        "# BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VyKmMR0nohC",
        "colab_type": "code",
        "outputId": "872c7315-474d-40d8-d6bf-cd52c0190b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\n",
        "from pytorch_transformers import RobertaForSequenceClassification\n",
        "bert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "#roberta_model_class = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:33<00:00, 12271014.95B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGVEs6t3n3M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSBgiQeZn54v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_02 = partial(accuracy_thresh, thresh=0.25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AITV5ZQn91H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = bert_model_class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOWgsFZon_bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.callbacks import *\n",
        "\n",
        "learner = Learner(\n",
        "    databunch_1, model,\n",
        "    loss_func=loss_func, model_dir='/temp/model', metrics=acc_02,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vzku2vXqF03",
        "colab_type": "code",
        "outputId": "99ff1e03-8802-4d9e-bc6c-3fc8a4f24095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "4//3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTG5y_xnoBtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_clas_split(self) -> List[nn.Module]:\n",
        "    \n",
        "    bert = model.bert\n",
        "    embedder = bert.embeddings\n",
        "    pooler = bert.pooler\n",
        "    encoder = bert.encoder\n",
        "    classifier = [model.dropout, model.classifier]\n",
        "    n = len(encoder.layer)//3\n",
        "    print(n)\n",
        "    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n",
        "    return groups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj09f0croFSb",
        "colab_type": "code",
        "outputId": "9c26364f-8678-441e-fe0c-ee5be2d44dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = bert_clas_split(model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T778vMkzoG5g",
        "colab_type": "code",
        "outputId": "28037754-7958-4da7-8cb9-b1ec06822ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.split([x[0], x[1], x[2], x[3], x[5]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (4211 items)\n",
              "x: TextList\n",
              "[CLS] according to gran , the company has no plans to move all production to russia , although that is where the company is growing [SEP],[CLS] techno ##polis plans to develop in stages an area of no less than 100 , 000 square meters in order to host companies working in computer technologies and telecommunications , the statement said [SEP],[CLS] with the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profit ##ability [SEP],[CLS] according to the company ' s updated strategy for the years 2009 - 2012 , bas ##ware targets a long - term net sales growth in the range of 20 % - 40 % with an operating profit margin of 10 % - 20 % of net sales [SEP],[CLS] for the last quarter of 2010 , component ##a ' s net sales doubled to eu ##r ##13 ##1 ##m from eu ##r ##7 ##6 ##m for the same period a year earlier , while it moved to a zero pre - tax profit from a pre - tax loss of eu ##r ##7 ##m [SEP]\n",
              "y: MultiCategoryList\n",
              "label_neutral,label_neutral,label_positive,label_positive,label_positive\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (843 items)\n",
              "x: TextList\n",
              "[CLS] in the fourth quarter of 2008 , net sales increased by 2 % to eu ##r 1 , 050 . 7 mn from eu ##r 1 , 02 ##7 . 0 mn in the fourth quarter of 2007 [SEP],[CLS] the contract value amounts to eu ##r 2 . 4 million [SEP],[CLS] ` ` demand for sports equipment was good in 2005 [SEP],[CLS] tie ##lin ##ja generated net sales of 7 . 5 ml ##n euro $ 9 . 6 ml ##n in 2005 [SEP],[CLS] ` nordic infrastructure construction is one of our strategic growth areas [SEP]\n",
              "y: MultiCategoryList\n",
              "label_positive,label_neutral,label_positive,label_neutral,label_neutral\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f4ec261c6a8>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='/temp/model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (3): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): Dropout(p=0.1)\n",
              "  (1): Linear(in_features=768, out_features=3, bias=True)\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUa4cEVApTyZ",
        "colab_type": "code",
        "outputId": "3a848a86-d998-4555-e7e3-fbfea56f7e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learner.lr_find()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcPz9hcUrr7t",
        "colab_type": "code",
        "outputId": "a23bca2b-c492-4a53-fd9d-d21ec493937d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learner.recorder.plot()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa9ysnx4yd7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P11rsWm_zLHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('head')\n",
        "learner.load('head')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUsLHziAArSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-2)\n",
        "learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3,1e-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMJirmzaBCsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('head-2')\n",
        "learner.load('head-2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivbw8chwBHPk",
        "colab_type": "code",
        "outputId": "149d6c4a-76e4-46d2-ae51-86c2efdb2e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "learner.unfreeze()\n",
        "learner.lr_find()\n",
        "learner.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
            "Min numerical gradient: 3.98E-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd_vECKCBKni",
        "colab_type": "code",
        "outputId": "08254bf1-6cc9-4e0b-86b3-45ee33449342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3,1e-1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy_thresh</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.412623</td>\n",
              "      <td>0.277589</td>\n",
              "      <td>0.832345</td>\n",
              "      <td>15:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.250688</td>\n",
              "      <td>0.200606</td>\n",
              "      <td>0.915382</td>\n",
              "      <td>15:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsloJxjmFW2B",
        "colab_type": "code",
        "outputId": "e7df6e20-f348-4c00-f750-186110d03e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "text = 'The company slipped to an operating loss of EUR 2.6 million from a profit of EUR 1.3 million'\n",
        "learner.predict(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MultiCategory label_negative,\n",
              " tensor([1., 0., 0.]),\n",
              " tensor([0.7159, 0.1642, 0.2504]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0roxTr5OHWx",
        "colab_type": "code",
        "outputId": "00d26784-10b9-4136-a7f8-5a627799a61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "text =  \"The contract value amounts to EUR 2.4 million\"\n",
        "learner.predict(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MultiCategory label_neutral,\n",
              " tensor([0., 1., 0.]),\n",
              " tensor([0.0018, 0.9982, 0.0016]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXg7jDjMNmEG",
        "colab_type": "code",
        "outputId": "bf362bee-0809-4fe6-8f0d-9077ca7e4fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>article_time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>Apple Inc’s Best Years Are Behind It — Prepare...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>(NASDAQ: AAPL )? It's a very real possibility</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>The company is running on the fumes of innova...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>It's now time to seriously consider betting a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>Paul Mampilly, my colleague over at Banyan Hil...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               Text\n",
              "0           0  ...  Apple Inc’s Best Years Are Behind It — Prepare...\n",
              "1           0  ...      (NASDAQ: AAPL )? It's a very real possibility\n",
              "2           0  ...   The company is running on the fumes of innova...\n",
              "3           0  ...   It's now time to seriously consider betting a...\n",
              "4           0  ...  Paul Mampilly, my colleague over at Banyan Hil...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwkGk2GgNqHm",
        "colab_type": "code",
        "outputId": "719bac6d-0349-46bd-89e2-c6ee47909fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>article_time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>Apple Inc’s Best Years Are Behind It — Prepare...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>(NASDAQ: AAPL )? It's a very real possibility</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>The company is running on the fumes of innova...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>It's now time to seriously consider betting a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-01-02 09:44:59</td>\n",
              "      <td>Paul Mampilly, my colleague over at Banyan Hil...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               Text\n",
              "0           0  ...  Apple Inc’s Best Years Are Behind It — Prepare...\n",
              "1           0  ...      (NASDAQ: AAPL )? It's a very real possibility\n",
              "2           0  ...   The company is running on the fumes of innova...\n",
              "3           0  ...   It's now time to seriously consider betting a...\n",
              "4           0  ...  Paul Mampilly, my colleague over at Banyan Hil...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcIE79-COFTs",
        "colab_type": "code",
        "outputId": "04a3f34f-ce66-4ef3-c0a8-530cf0ebbcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test[\"Text\"][102]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3%, at $174'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7yK4SL-7Ti1",
        "colab_type": "code",
        "outputId": "3f519230-3518-4f65-851f-c0a484d8cadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "text =  test[\"Text\"][18]\n",
        "learner.predict(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(MultiCategory label_positive,\n",
              " tensor([0., 0., 1.]),\n",
              " tensor([0.1146, 0.1848, 0.7609]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B0WPrO88ObE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(x):\n",
        "  return learner.predict(x)[1].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ufpG_e0cRxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction_proba(x):\n",
        "  return learner.predict(x)[2].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkBS2BlH826R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(x):\n",
        "  if x[0] == 1:\n",
        "    return \"label_negative\"\n",
        "  elif x[1] == 1:\n",
        "    return \"label_neutral\"\n",
        "  elif x[2] == 1:\n",
        "    return \"label_positive\"\n",
        "  else:\n",
        "    return \"no label\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hvcbySv7YYy",
        "colab_type": "code",
        "outputId": "1165b635-8fe3-416a-eee0-22d4e72335ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "test[\"pred\"] = test[\"Text\"].apply(prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-d565b8814d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-d217cd71b6b9>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, item, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;34m\"Return predicted class, label and probabilities for `item`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrab_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpred_batch\u001b[0;34m(self, ds_type, batch, reconstruct)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mcb_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_loss_func2activ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY8MTYrPimyd",
        "colab_type": "code",
        "outputId": "8ef41a89-53cc-427b-a56a-d21c7800541c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>article_time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>Why Apple,  Stock Gained 11% in 2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>Apple(NASDAQ: AAPL) finished 2016 strong, gai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>8%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>The nice increase toward the end of the year ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>9% rise during the year, according to data pro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ...                                               Text\n",
              "0          0  ...               Why Apple,  Stock Gained 11% in 2016\n",
              "1          1  ...   Apple(NASDAQ: AAPL) finished 2016 strong, gai...\n",
              "2          2  ...                                                 8%\n",
              "3          3  ...   The nice increase toward the end of the year ...\n",
              "4          4  ...  9% rise during the year, according to data pro...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4rTlwcPceNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test[\"pred_proba\"] = test[\"Text\"].apply(prediction_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E43u1tUUXiUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEv2EOkz8bBk",
        "colab_type": "code",
        "outputId": "72183dc4-b842-4664-8607-c6e1f343c68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "test[\"true\"] = test[[\"label_negative\",\"label_neutral\",\"label_positive\"]].idxmax(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Lp9fHydo6i",
        "colab_type": "code",
        "outputId": "1c216c79-5744-46e6-c3c2-302d41a09ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test[\"pred\"][96])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha20lFLw-Led",
        "colab_type": "code",
        "outputId": "91a876bd-3f81-47e3-da7b-c86bc8734140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label_negative</th>\n",
              "      <th>label_neutral</th>\n",
              "      <th>label_positive</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_proba</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>In the fourth quarter of 2008 , net sales incr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 1.0]</td>\n",
              "      <td>[0.07034596800804138, 0.039265722036361694, 0....</td>\n",
              "      <td>label_positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3305</th>\n",
              "      <td>The contract value amounts to EUR 2.4 million</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 1.0, 0.0]</td>\n",
              "      <td>[0.014253164641559124, 0.9578186273574829, 0.0...</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>`` Demand for sports equipment was good in 2005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 0.0, 1.0]</td>\n",
              "      <td>[0.04201382398605347, 0.07096993923187256, 0.9...</td>\n",
              "      <td>label_positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>Tielinja generated net sales of 7.5 mln euro $...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 1.0, 0.0]</td>\n",
              "      <td>[0.026005534455180168, 0.9632948040962219, 0.0...</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2303</th>\n",
              "      <td>` Nordic infrastructure construction is one of...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.0, 0.0, 1.0]</td>\n",
              "      <td>[0.02008826471865177, 0.22700658440589905, 0.8...</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  ...            true\n",
              "96    In the fourth quarter of 2008 , net sales incr...  ...  label_positive\n",
              "3305     The contract value amounts to EUR 2.4 million   ...   label_neutral\n",
              "811    `` Demand for sports equipment was good in 2005   ...  label_positive\n",
              "463   Tielinja generated net sales of 7.5 mln euro $...  ...   label_neutral\n",
              "2303  ` Nordic infrastructure construction is one of...  ...   label_neutral\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiQ759fzisK0",
        "colab_type": "code",
        "outputId": "1089f3bc-17fe-493a-b247-e7b1a934bbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "test = test[[\"sentence\",\"pred\",\"pred_proba\",\"true\"]]\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_proba</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>In the fourth quarter of 2008 , net sales incr...</td>\n",
              "      <td>[0.0, 0.0, 1.0]</td>\n",
              "      <td>[0.07034596800804138, 0.039265722036361694, 0....</td>\n",
              "      <td>label_positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3305</th>\n",
              "      <td>The contract value amounts to EUR 2.4 million</td>\n",
              "      <td>[0.0, 1.0, 0.0]</td>\n",
              "      <td>[0.014253164641559124, 0.9578186273574829, 0.0...</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>`` Demand for sports equipment was good in 2005</td>\n",
              "      <td>[0.0, 0.0, 1.0]</td>\n",
              "      <td>[0.04201382398605347, 0.07096993923187256, 0.9...</td>\n",
              "      <td>label_positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>Tielinja generated net sales of 7.5 mln euro $...</td>\n",
              "      <td>[0.0, 1.0, 0.0]</td>\n",
              "      <td>[0.026005534455180168, 0.9632948040962219, 0.0...</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2303</th>\n",
              "      <td>` Nordic infrastructure construction is one of...</td>\n",
              "      <td>[0.0, 0.0, 1.0]</td>\n",
              "      <td>[0.02008826471865177, 0.22700658440589905, 0.8...</td>\n",
              "      <td>label_neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  ...            true\n",
              "96    In the fourth quarter of 2008 , net sales incr...  ...  label_positive\n",
              "3305     The contract value amounts to EUR 2.4 million   ...   label_neutral\n",
              "811    `` Demand for sports equipment was good in 2005   ...  label_positive\n",
              "463   Tielinja generated net sales of 7.5 mln euro $...  ...   label_neutral\n",
              "2303  ` Nordic infrastructure construction is one of...  ...   label_neutral\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHJYxqMVi0Kd",
        "colab_type": "code",
        "outputId": "f8fb9377-6cd7-4d6f-9eb4-176224bef25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUwVOYPEi9RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv('results_aapl_4.csv')\n",
        "!cp results_aapl_4.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiyHuvT0jODb",
        "colab_type": "code",
        "outputId": "6b344663-0edc-4e0c-f7ed-4b8ba27417a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>article_time</th>\n",
              "      <th>Text</th>\n",
              "      <th>pred_proba</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>Why Apple,  Stock Gained 11% in 2016.</td>\n",
              "      <td>[0.09973956644535065, 0.2574954330921173, 0.79...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>Apple(NASDAQ: AAPL) finished 2016 strong, gain...</td>\n",
              "      <td>[0.15682728588581085, 0.1222851425409317, 0.84...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>The nice increase toward the end of the year h...</td>\n",
              "      <td>[0.20763783156871796, 0.09087919443845749, 0.8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>While the increase is notable, investors shoul...</td>\n",
              "      <td>[0.0588444322347641, 0.4770902991294861, 0.631...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2017-01-03 10:26:00</td>\n",
              "      <td>Image source: Apple.</td>\n",
              "      <td>[0.09454485028982162, 0.946122407913208, 0.057...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         pred_proba\n",
              "0           0  ...  [0.09973956644535065, 0.2574954330921173, 0.79...\n",
              "1           1  ...  [0.15682728588581085, 0.1222851425409317, 0.84...\n",
              "2           2  ...  [0.20763783156871796, 0.09087919443845749, 0.8...\n",
              "3           3  ...  [0.0588444322347641, 0.4770902991294861, 0.631...\n",
              "4           4  ...  [0.09454485028982162, 0.946122407913208, 0.057...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSBTKI0Nllal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}